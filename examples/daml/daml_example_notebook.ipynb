{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML Dataset Evaluation with NRTK Perturber\n",
    "This is a notebook demonstrating how sensor and scenario perturbations from the Natural Robustness Toolkit (NRTK) affect the DP Divergence metric generated using the Data-Analysis Metrics Library (DAML).\n",
    "\n",
    "## Table Of Contents\n",
    "\n",
    "* [Environment Setup](#environment-setup)\n",
    "* [Perturber Instantiation](#perturber-instantiation)\n",
    "* [Dataset Creation](#dataset-creation)\n",
    "* [Load The Model](#load-the-model)\n",
    "* [Evaluate Datasets](#evaluate-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup <a name=\"environment-setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we install and import dependencies. This notebook assumes that the user is operating in an environment consistent with the NRTK poetry file and with NRTK already installed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -qU pip\n",
    "print(\"Installing matplotlib...\")\n",
    "!{sys.executable} -m pip install -q matplotlib\n",
    "print(\"Installing alibi_detect...\")\n",
    "!{sys.executable} -m pip install -q alibi_detect==0.11.5\n",
    "print(\"Installing daml...\")\n",
    "!{sys.executable} -m pip install -q daml\n",
    "# Remove opencv-python, which requires libGL, which we don't require here, and replace with opencv-python-headless\n",
    "print(\"Installing headless OpenCV...\")\n",
    "!{sys.executable} -m pip uninstall -qy opencv-python opencv-python-headless  # make sure they're both gone.\n",
    "!{sys.executable} -m pip install -q opencv-python-headless\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two helper functions. `stretch_contrast_convert_8bit` is a function that stretches the contrast to the full capacity of the image. This is used to ensure the colors in the images match before running them through the DP Divergence calculation. The second function is `crop_image` which takes in a single image in and splits it into a set of output tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_contrast_convert_8bit(img, perc=[0.1, 99.9]):\n",
    "    img = img.astype(float)\n",
    "    img = img - np.percentile(img.ravel(), perc[0])\n",
    "    img = img/(np.percentile(img.ravel(), perc[1])/255)\n",
    "    img = np.clip(img, 0, 255)\n",
    "    return np.round(img).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, input_size=512, output_size=64):\n",
    "\n",
    "  \"\"\"Crops a input_sizexinput_sizex3 image into a set of output_sizexoutput_sizex3 images.\n",
    "\n",
    "  Args:\n",
    "    img: A numpy array representing an image.\n",
    "\n",
    "    input_size: the edge size of a square image\n",
    "\n",
    "    output_size: the desired edge size of the cropped image\n",
    "\n",
    "  Returns:\n",
    "    A list of output_sizexoutput_sizex3 numpy arrays representing the cropped images.\n",
    "  \"\"\"\n",
    "\n",
    "  cropped_images = []\n",
    "  \n",
    "  for i in range(0, input_size, output_size):\n",
    "    for j in range(0, input_size, output_size):\n",
    "      cropped_images.append(img[i:i + output_size, j:j + output_size, :])\n",
    "\n",
    "  return cropped_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturber Instantiation <a name=\"perturber-instantiation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define the parameters of both the sensor and scenario objects that NRTK uses to perturb the images. From this base set of parameters, we create a set of perturbers with varying focal length and ground range parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrtk.impls.perturb_image.pybsm.scenario import PybsmScenario\n",
    "from nrtk.impls.perturb_image.pybsm.sensor import PybsmSensor\n",
    "import pybsm\n",
    "\n",
    "#telescope focal length (m)\n",
    "f=0.6\n",
    "# Telescope diameter (m)\n",
    "D=0.0648\n",
    "\n",
    "#detector pitch (m)\n",
    "p=2e-05\n",
    "#Optical system transmission, red  band first (m)\n",
    "optTransWavelengths = np.array([0.85-.15,0.85+.15])*1.0e-6\n",
    "\n",
    "L3 = PybsmSensor(\n",
    "    # required\n",
    "    name                = 'L32511x',\n",
    "    D                   = D, # Telescope diameter (m)\n",
    "    f                   = f, #telescope focal length (m)\n",
    "    px                  = p, #detector pitch (m)\n",
    "    optTransWavelengths = optTransWavelengths, #Optical system transmission, red  band first (m)\n",
    "\n",
    "    # optional\n",
    "    opticsTransmission  = 0.5*np.ones(optTransWavelengths.shape[0]), #guess at the full system optical transmission (excluding obscuration)\n",
    "    eta                 = 0.4, #guess\n",
    "    wx                  = p, #detector width is assumed to be equal to the pitch\n",
    "    wy                  = p, #detector width is assumed to be equal to the pitch\n",
    "    intTime             = 30.0e-3, #integration time (s) - this is a maximum, the actual integration time will be, determined by the well fill percentage\n",
    "    darkCurrent         = pybsm.darkCurrentFromDensity(1e-5,p,p), #dark current density of 1 nA/cm2 guess, guess mid range for a silicon camera\n",
    "    readNoise           = 25.0, #rms read noise (rms electrons)\n",
    "    maxN                = 96000.0, #maximum ADC level (electrons)\n",
    "    bitdepth            = 11.9, #bit depth\n",
    "    maxWellFill         = .6, #maximum allowable well fill (see the paper for the logic behind this)\n",
    "    sx                  = 0.25*p/f, #jitter (radians) - The Olson paper says that its \"good\" so we'll guess 1/4 ifov rms\n",
    "    sy                  = 0.25*p/f, #jitter (radians) - The Olson paper says that its \"good\" so we'll guess 1/4 ifov rms\n",
    "    dax                 = 100e-6, #drift (radians/s) - again, we'll guess that it's really good\n",
    "    day                 = 100e-6, #drift (radians/s) - again, we'll guess that it's really good\n",
    "    qewavelengths       = np.array([.3, .4, .5, .6, .7, .8, .9, 1.0, 1.1])*1.0e-6,\n",
    "    qe                  = np.array([0.05, 0.6, 0.75, 0.85, .85, .75, .5, .2, 0])\n",
    ")\n",
    "\n",
    "S = PybsmScenario(\n",
    "    name='niceday',\n",
    "    ihaze=1, #weather model\n",
    "    altitude=9000.0, #sensor altitude\n",
    "    groundRange=0.0, #range to target\n",
    "\n",
    "    aircraftSpeed=100.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrtk.impls.perturb_image.pybsm.perturber import PybsmPerturber\n",
    "\n",
    "#create focal length perturbers\n",
    "L3.f = 0.6\n",
    "fl_perturber_0 = PybsmPerturber(L3, S)\n",
    "L3.f = 0.4\n",
    "fl_perturber_1 = PybsmPerturber(L3, S)\n",
    "L3.f = 0.2\n",
    "fl_perturber_2 = PybsmPerturber(L3, S)\n",
    "L3.f = 0.1\n",
    "fl_perturber_3 = PybsmPerturber(L3, S)\n",
    "fl_perturbers = [fl_perturber_0, fl_perturber_1, fl_perturber_2, fl_perturber_3]\n",
    "\n",
    "#reset f for the ground range perturbers\n",
    "L3.f = 0.6\n",
    "\n",
    "#create ground range perturbers\n",
    "S.groundRange = 0\n",
    "gr_perturber_0 = PybsmPerturber(L3, S)\n",
    "S.groundRange = 5000\n",
    "gr_perturber_1 = PybsmPerturber(L3, S)\n",
    "S.groundRange = 10000\n",
    "gr_perturber_2 = PybsmPerturber(L3, S)\n",
    "S.groundRange = 15000\n",
    "gr_perturber_3 = PybsmPerturber(L3, S)\n",
    "gr_perturbers = [gr_perturber_0, gr_perturber_1, gr_perturber_2, gr_perturber_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation <a name=\"dataset-creation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the perturbers we load in a set of 10 xView images, run them through each of the perturbers, and crop the images to get a set of 64x64 sized perturbed output images. Additionally we load in a set of 10 unperturbed xView images that are cropped to the same size as the perturbed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare focal length perturbed dataset\n",
    "im_folder = \"./data/perturbed/\"\n",
    "crop_size = 64\n",
    "reshape = (512, 512)\n",
    "fl_datasets = []\n",
    "\n",
    "for perturber in fl_perturbers:\n",
    "    perturbed_ims = []\n",
    "    for file in os.listdir(im_folder):\n",
    "        im = cv2.imread(im_folder + file)\n",
    "        im = perturber(im, {\"img_gsd\": 0.3})\n",
    "        im = cv2.resize(im, reshape, interpolation=cv2.INTER_AREA)\n",
    "        im = stretch_contrast_convert_8bit(im)\n",
    "        cropped_images = crop_image(im, im.shape[0], crop_size)\n",
    "        perturbed_ims.extend(cropped_images)\n",
    "    fl_perturbed_ims = np.array(perturbed_ims)\n",
    "    fl_datasets.append(fl_perturbed_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare ground range perturbed dataset\n",
    "im_folder = \"./data/perturbed/\"\n",
    "perturbed_ims = []\n",
    "gr_datasets = []\n",
    "\n",
    "for perturber in gr_perturbers:\n",
    "    perturbed_ims = []\n",
    "    for file in os.listdir(im_folder):\n",
    "        im = cv2.imread(im_folder + file)\n",
    "        im = perturber(im, {\"img_gsd\": 0.3})\n",
    "        im = cv2.resize(im, reshape, interpolation=cv2.INTER_AREA)\n",
    "        im = stretch_contrast_convert_8bit(im)\n",
    "        cropped_images = crop_image(im, im.shape[0], crop_size)\n",
    "        perturbed_ims.extend(cropped_images)\n",
    "    gr_perturbed_ims = np.array(perturbed_ims)\n",
    "    gr_datasets.append(gr_perturbed_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare unperturbed dataset\n",
    "im_folder = \"./data/unperturbed/\"\n",
    "crop_size = 64\n",
    "unperturbed_ims = []\n",
    "for file in os.listdir(im_folder):\n",
    "    im = cv2.imread(im_folder + file)#, cv2.COLOR_BGR2GRAY)\n",
    "    im = stretch_contrast_convert_8bit(im)\n",
    "    cropped_images = crop_image(im, im.shape[0], crop_size)\n",
    "    unperturbed_ims.extend(cropped_images)\n",
    "\n",
    "unperturbed_ims = np.array(unperturbed_ims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model <a name=\"load-the-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to finish the preparation steps we load in an encoder extracted from an instance of the AE class from `daml.metrics.outlier_detection` that has been trained on cropped chips from xView images. The encoder that is loaded in was trained on the entire set of xView images located in the folder here: https://gitlab.jatic.net/jatic/demos/-/tree/main/increment2/supporting_files/satellite_te/xview/chipped?ref_type=heads\n",
    "\n",
    "These images were then cropped into 64x64 sized tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load encoder from .pkl file\n",
    "encoder_file = \"./model/xview_encoder.pkl\"\n",
    "\n",
    "with open(encoder_file, 'rb') as outp:\n",
    "    encoder = pickle.load(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Datasets <a name=\"evaluate-datasets\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we generate features for the set of unperturbed images, focal length perturbed images, and ground range perturbed images.\n",
    "Once we have the encodings we can compare the perturbed image features against the unperturbed image features to get the DP Divergence values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from daml.metrics.divergence.aria import HP_FNN  # type: ignore\n",
    "\n",
    "#Run images through encoder, extract features, and compare with dpdivergence\n",
    "metric = HP_FNN()\n",
    "fl_results = []\n",
    "gr_results = []\n",
    "\n",
    "unperturbed_feats = encoder(unperturbed_ims)\n",
    "for i in range(len(fl_datasets)):\n",
    "    fl_perturbed_feats = encoder(fl_datasets[i])\n",
    "    gr_perturbed_feats = encoder(gr_datasets[i])\n",
    "\n",
    "    fl_dp_divergence = metric.evaluate(dataset_a=unperturbed_feats, dataset_b=fl_perturbed_feats)\n",
    "    gr_dp_divergence = metric.evaluate(dataset_a=unperturbed_feats, dataset_b=gr_perturbed_feats)\n",
    "\n",
    "    fl_results.append(fl_dp_divergence.dpdivergence)\n",
    "    gr_results.append(gr_dp_divergence.dpdivergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the DP Divergence values for each perturbation metric when measured against the unperturbed images. The red dashed line indicates the parameter value of the simulated image with the estimated parameters of the camera, and the DP Divergence generally increases as each metric gets further away from the starting parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = [0.6, 0.4, 0.2, 0.1]\n",
    "\n",
    "plt.title(\"Focal Length Perturbation vs DP Divergence\")\n",
    "plt.xlabel(\"Focal Length\")\n",
    "plt.ylabel(\"DP Divergence\")\n",
    "plt.vlines(x=[0.6], ymin=0.45, ymax=0.7, colors='red', ls='--', lw=2, label='Original Image Focal Length')\n",
    "\n",
    "plt.plot(f, fl_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = [0, 5000, 10000, 15000]\n",
    "\n",
    "plt.title(\"Ground Range Perturbation vs DP Divergence\")\n",
    "plt.xlabel(\"Ground Range\")\n",
    "plt.ylabel(\"DP Divergence\")\n",
    "plt.vlines(x=[0], ymin=0.45, ymax=0.6, colors='red', ls='--', lw=2, label='Original Image Ground Range')\n",
    "\n",
    "plt.plot(gr, gr_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
